{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Mining:<br>Statistical Modeling and Learning from Data\n",
    "\n",
    "## Dr. Ciro Cattuto<br>Dr. Laetitia Gauvin<br>Dr. Andr√© Panisson\n",
    "\n",
    "### Exercises - Support Vector Machines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Graphic illustration of Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data\n",
    "\n",
    "In the following code, we generated 100 samples of a synthetic dataset using the **sklearn.datasets.make_classification** function.\n",
    "\n",
    "This dataset has 2 classes (**n_classes=2**) and 2 features (**n_features=2**). It means that $y \\in \\{0,1\\}$ and $X \\in \\mathbb{R}^2$\n",
    "\n",
    "The parameter **class_sep** controls the separability of the class, and in this example, we set this to return a dataset that is linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a colormap for the points\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100, n_classes=2,\n",
    "                                    n_features=2, n_redundant=0, n_informative=2,\n",
    "                                    random_state=1, n_clusters_per_class=1, class_sep=1.4)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "figure(figsize=(8, 6))\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "\n",
    "# set the graph limits\n",
    "ylim(y_min, y_max)\n",
    "xlim(x_min, x_max);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot the decision boundary and the support vectors of a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(model, X):\n",
    "    # create a mesh of points that cover the full graph area\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # use the classifier to predict the class of each mesh point\n",
    "    Z = clf.decision_function(X_grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    figure(figsize=(8, 6))\n",
    "\n",
    "    # plot the decision boundary\n",
    "    norm = plt.cm.colors.Normalize(vmax=abs(Z).max(), vmin=-abs(Z).max())\n",
    "    contourf(xx, yy, Z, 12, cmap=plt.cm.RdBu, alpha=.8, norm=norm)\n",
    "    # plt.colorbar()\n",
    "\n",
    "    # plot the decision hyper-planes\n",
    "    contour(xx, yy, Z, colors=['k', 'k', 'k'],\n",
    "            linestyles=['--', '-', '--'],\n",
    "            levels=[-1., 0, 1.])\n",
    "\n",
    "    # plot the dataset points\n",
    "    scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "\n",
    "    # plot the support vectors\n",
    "    SV = clf.support_vectors_\n",
    "    scatter(SV[:, 0], SV[:, 1], c=y[clf.support_],\n",
    "            cmap=cm_bright, s=500, marker='x')\n",
    "\n",
    "    # set the graph limits\n",
    "    ylim(y_min, y_max)\n",
    "    xlim(x_min, x_max);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Linear kernel with high penalty ($C \\rightarrow \\infty$)\n",
    "\n",
    "The parameter **C** controls the penalty of the error terms for a soft-margin SVM classifier.\n",
    "\n",
    "Setting **C** to a high value means that the classifier will try to maximize the margin with a high penalty for misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1e12)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plot_boundary(clf, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Linear kernel with low penalty ($C \\rightarrow 0$)\n",
    "\n",
    "Setting **C** to a low value means that the classifier will try to maximize the margin with a small penalty for misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1e-1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plot_boundary(clf, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: RBF kernel with high penalty ($C \\rightarrow \\infty$) and small $\\gamma$:\n",
    "\n",
    "The Radial Basis Function kernel is given by the function $\\exp(-\\gamma \\left|x-x'\\right|^2)$.\n",
    "\n",
    "When training an SVM with the RBF kernel, two parameters must be considered: $C$ and $\\gamma$. The parameter $C$, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low $C$ makes the decision surface smooth, while a high $C$ aims at classifying all training examples correctly. $\\gamma$ defines how much influence a single training example has. The larger $\\gamma$ is, the closer other examples must be to be affected.\n",
    "\n",
    "Setting **C** to a high value means that the classifier will try to maximize the margin with a high penalty for misclassified points, and will try to classify all training examples correctly.\n",
    "\n",
    "The parameter **gamma** is the coefficient of the RBF function. Setting **gamma** to a small value means that just a few training examples will have influence in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', gamma=1e-1, C=1e12)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plot_boundary(clf, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: RBF kernel with high penalty ($C \\rightarrow \\infty$) and high $\\gamma$ (e.g., $\\gamma = 20$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Check the behavior of the SVM classifiers in the case when the dataset is not linearly separable.\n",
    "\n",
    "First, create a new dataset that is not linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100, n_classes=2,\n",
    "                                    n_features=2, n_redundant=0, n_informative=2,\n",
    "                                    random_state=1, n_clusters_per_class=1, class_sep=0.2)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "figure(figsize=(8, 6))\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "\n",
    "# set the graph limits\n",
    "ylim(y_min, y_max)\n",
    "xlim(x_min, x_max);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check the behavior of the SVM classifiers with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear kernel with high penalty ($C \\rightarrow \\infty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear kernel with low penalty ($C \\rightarrow 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF kernel with high penalty ($C \\rightarrow \\infty$) and small $\\gamma$ (e.g., $\\gamma = 0.1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF kernel with high penalty ($C \\rightarrow \\infty$) and high $\\gamma$ (e.g., $\\gamma = 20$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Classify digits with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Digits dataset\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n",
    "\n",
    "Load the features in a variable with name $\\mathbf{X}$ and the target in a variable with name $\\mathbf{y}$.\n",
    "\n",
    "**Attention**: This dataset is composed of 1797 images of $8 \\times 8$ pixels. You have to transform it in a matrix of 1797 samples and 64 features ($1797 \\times 64$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# plot the first 21 samples\n",
    "for index, (image, label) in enumerate(zip(digits.images[:21], digits.target[:21])):\n",
    "    plt.subplot(3, 7, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('%i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.images.reshape(digits.images.shape[0], -1)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we have to classify the dataset in 10 different classes. We can use any classifier that is able to classify in 2 different classes to classify in n different classes. There are 2 main strategies for this:\n",
    "\n",
    "- One-versus-one: http://scikit-learn.org/stable/modules/multiclass.html#one-vs-one\n",
    "- One-versus-rest: http://scikit-learn.org/stable/modules/multiclass.html#one-vs-the-rest\n",
    "\n",
    "The implementation of SVM classifier (SVC) in scikit-learn uses one-vs-one strategy:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html#multi-class-classification\n",
    "\n",
    "Next, create a SVM classifier with **rbf** kernel and default parameters.\n",
    "Then, create a list named **`gammas`** with values for the rbf coefficient (gamma) starting from $10^{-5}$ to $10^{-0.5}$, spaced evenly on a log scale. For each value of gamma, set the model parameter **`gamma`** to this value, calculate the 10-fold cross-validation scores, and add the average score to a list with name **`scores`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot the scores for each gamma, and print the best gamma and the associated score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best gamma:', gammas[np.argmax(scores)])\n",
    "print('Best score:', scores[np.argmax(scores)])\n",
    "\n",
    "semilogx(gammas, scores)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('Score (accuracy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "Find both **gamma** and **C** that minimizes the error (or maximizes the score) using 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
